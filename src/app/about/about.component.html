<div class="about-container">
  <h1>About</h1>

  <h2>‚öôÔ∏è Engine Overview</h2>
  <p>
    The engine on this page is a direct port of the Java engine from
    <a href="https://www.codingame.com/multiplayer/bot-programming/tic-tac-toe/leaderboard?column=keyword&value=bortol" target="_blank" rel="noopener">CodinGame</a>,
    ranking about <strong>10th out of 9,600</strong> players and 1st among 1,000+ Java bots. It runs efficiently on the client side using <strong>WebAssembly</strong>. The engine's code is not disclosed, as it could hurt competitiveness on the CodinGame platform.
  </p>

  <h2>üß† AI Architecture</h2>
  <p>
    The engine uses an <em>NNUE</em> (Efficiently Updatable Neural Network) sized <code>396x128x1</code>, combined with a Monte Carlo Tree Search (MCTS) best-first search called
    <a href="https://www.codingame.com/playgrounds/55004/best-first-minimax-search-with-uct" target="_blank" rel="noopener">jacek-max</a>. The neural network weights are quantized to integers to fit within CodinGame's 100,000 character limit, featuring a hidden layer of size 128. The neural network evaluates the position from player to move perspective (therefore 2 accumulators are maintained during the search).
  </p>
  <p>
    An alpha-beta version was attempted but proved less competitive in my implementation..
  </p>

  <h2>üîÑ Training Process</h2>
  <p>
    The training followed an <strong>AlphaZero-style self-play</strong> approach, where the engine learned by playing against itself, starting from random weights. No prior knowledge of the game rules or strategies was encoded, the engine discovered effective play purely through self-play and reinforcement learning.
  </p>
  <p>
    Training was done with a custom pipeline using a 10 million sample buffer over 200 iterations. The learning target mixed the game result and search evaluation equally (<code>0.5 √ó game result + 0.5 √ó search eval</code>), with 200 search iterations per move.
  </p>
  <p>
    During training, the first moves were selected using weighted randomness based on visit counts to introduce some variability. A batch size of 256 was used, and the learning rate gradually decreased from <code>1e-4</code> to <code>1e-9</code>. The training data was augmented with all possible symmetries.
  </p>

  <h2>üöÄ Performance and Tuning</h2>
  <p>
    The engine is optimized for CodinGame‚Äôs environment, focusing on deep rather than wide searches. Despite longer move times, some paths may be missed.
  </p>

  <h2>‚úâÔ∏è Contact</h2>
  <p>If you'd like to reach out, feel free to contact me on Discord: <strong>bortol#2802</strong>.</p>
</div>
